# Data Pipeline ğŸš

This is a simple project to run query metrics for quick data exploration in a **local environment**.\
It simulates an **OLAP-like analytical environment** using **DuckDB** and executes analytical queries via Python scripts â€” supporting both **pure DuckDB SQL** and **DuckDB combined with Apache Beam** for batch processing.

âœ… **Google Cloud Platform (GCP) Ready**: the project structure makes it easy to migrate to GCP if needed.

---

## ğŸ“¦ Project Structure

```
.
â”œâ”€â”€ db/              # DuckDB database files
â”œâ”€â”€ files/           # Raw CSV input files
â”œâ”€â”€ storage/         # Output files (queries results)
â”œâ”€â”€ utils/           # Utility scripts (saving outputs, helpers)
â”œâ”€â”€ create_database.py
â”œâ”€â”€ run_query.py
â”œâ”€â”€ run_beam.py
â”œâ”€â”€ main_runner.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ docker-compose.yml
```

---

## ğŸŸ£ Full Installation (Docker)

### Requirements:

- Docker and Docker Compose

### How to Run:

```bash
docker-compose build
docker-compose up
```

### If stuck or no output appears 

you can manually run the script inside the container:

```bash
docker-compose build
docker-compose up
```

```bash
docker exec -it rj_smtr_container bash
python main_runner.py
```


This will automatically:

- Create the database if it doesn't exist
- Prompt for query selection via CLI
- Run the selected query and save outputs in `/storage/`

---

## ğŸŸ¡ Simple Installation (Local Python)

### Requirements:

- Python 3.11+
- Pip

### How to Run:

```bash
pip install -r requirements.txt
python main_runner.py
```



This method runs everything locally without Docker, using your systemâ€™s Python environment.

---

## âœ… Summary

- Supports analytical queries via DuckDB and Apache Beam
- Easily adaptable for GCP Dataflow if required
- Outputs query results into `/storage/` for easy export*

*still in development

